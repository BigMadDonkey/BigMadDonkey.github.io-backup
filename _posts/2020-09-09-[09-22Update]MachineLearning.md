---
title: 机器学习个人笔记
author: MadDonkey
categories: [note, machineLearning]
layout: post
---
这是在学校上机器学习课的个人笔记。
~~本来在Typora上可以直接用“[TOC}”添加内容目录的，用jekyll就不行，使用jekyll-toc插件也有问题，淦矣~~

结果只好自己手动写链接锚点了。

注：涉及到图片的位置可能会发现一个不能正确加载的img标签，这是因为jekyll生成的目录与本地目录不一致所致，那个不能正确加载的图片，通过github 预览以及本地查看markdown文件时可以替代博客网站上的图片显示。

### 目录

1. **<a href = "#数学基础">数学基础</a>**

2. **<a href="#2020/09/10">2020/09/10</a>**
   <a href="#什么是机器学习">什么是机器学习</a>
   <a href="#机器学习的一般泛型">机器学习的一般泛型</a>
   <a href="#机器学习理论">机器学习理论</a>
   <a href="#决策树">决策树</a>
   
3. <a href="#2020/09/15">**2020/09/15**</a>
   <a href="#信息熵">信息熵</a>
   <a href="#条件熵">条件熵</a>
   <a href="#样本熵">样本熵</a>
   
4. **<a href="#2020/09/17">2020/09/17</a>**
     <a href="#样本属性缺失">样本属性缺失</a>
     
5. **<a href="#2020/09/22">2020/09/22</a>**

     

  <br />

## <a id="数学基础">数学基础</a>

数学真的很重要啊...可惜，大一大二的数学基础课程全都拿去考试用了。没办法，需要记一记的就记一记吧。

### 线性代数

他妈的！每次想到学校的线性代数教材安排的如此狗屁，都必须骂出一句话才行！不然在肚子里要憋坏了。这种奇妙的安排，不是祸害国内的大学生吗！！！

<center>    <img src="{{'assets/postResources/image-20200917200739590.png'|relative_url}}" alt="淦" />    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">他妈的，你这样做...</div> </center>

不过后来了解到许多其他大学也是这样的🙂那没事了。





### 概率论与数理统计



#### beta分布



## <a id="2020/09/10">2020/09/10</a>

### <a id="什么是机器学习">什么是机器学习</a>

研究计算机如何模拟/实现人的学习行为。

机器学习是人工智能的核心。其应用遍及NLP，对象识别，文本挖掘，生物信息学等。

给定训练样本集D={Xi，Yi}，输出空间（标记空间）Y（监督学习特有）。每个样本有多个属性，以每个属性为维度张成的空间称为样本空间（属性空间）。寻找一个能够建立起属性空间上一点到输出空间的映射的过程就是机器学习。样本对应的输出空间上的值一般称为标记(label)。

模型（也即函数）F(;θ)（也可作F(\|θ)，在机器学习理论中不区分），有时也称学习器（learner），就是机器学习的成果。学得模型后，使用其进行预测的过程称为测试，被预测的样本称为测试样本。

### <a id="机器学习的一般泛型">机器学习的一般泛型</a>

一般的机器学习类型：

- 监督学习
  数据集的样本都带有标记

- 无监督学习
  数据集的样本无标记

- 强化学习
  根据回报影响学习的策略

根据输出空间的形式不同，可分成：

- 离散型集合Y如Y={+1，-1}，称为**归类**。对于只涉及两个类别的二分类，通常称其中一个为正类，另一个为负类。
- 连续型集合Y如Y = R，称为**回归**。

除了得到一个可以预测label的模型，

### <a id="机器学习理论">机器学习理论：</a>

对于学习产生的模型（也可以说，函数F(;θ）），研究其

1. 一致性
2. 偏执与方差
3. 采样复杂性
4. 学习率（学习速度）
5. 收敛性
6. 误差界
7. 稳定性

……等等。通常假设样本空间全体样本服从一个“未知分布”D，我们获得的每个样本都是独立地从这个分布上采样获得的，称为独立同分布（iid）。

### 机器学习研究的目的

开发一种具有如下能力的系统：

- 表示
- 分类，聚类，识别
- 不确定条件下的推理（如电商app的商品推荐）
- 预测
- 对外界环境的反应

经过学习获得的模型，对应关于数据的**某种潜在规律**，称为**假设**（hypothesis），而样本空间的潜在规律本身称为**真相/真实**（ground-truth）。假设与真相的关系，就好比社会主义和共产主义，永远无法到达，但可以不断接近。

机器学习获得的模型，我们要使用它来根据样本空间中一般的输入预测目标变量的结果，这就是泛化。使用已有模型和属性参数去“预测”label，就是机器学习研究的意义所在！

可以把学习的过程看作在所有假设空间中搜索`function()`的过程。

### <a id="决策树">决策树</a>

决策树是一个简单的监督学习分类的非参数机器学习算法。

一颗决策树可以表示输入属性的任何函数。可以把它看作一个多元函数，多个属性就是自变量，标签就是因变量。决策树的每个内节点是属性，根据各样本的属性不同，把样本全集从根节点开始层层分成不同部分，到达一定阶段后就输出分类结果（叶节点）。

可以为每个样例建立一个从根到叶节点的路径，但是这样做泛化能力不强，就像备考时将题目的答案背下来，而不会做其他题一样（其实就是过拟合！）。因此应当找一个规模小一点的决策树，掌握样本的规律而非记录样本值。

对同一个训练数据集合，可能有多个决策树与之一致。

决策树能对数据进行**切分**。决策树的关键，毫无疑问就是选择节点，也就是**特征选择**了。将不同的属性安在不同的位置，对决策树的效果有很大影响。有的特征与分类的结果相关性较高，就更容易根据特征的不同筛选出具有不同label的样本。

比如下面的例子：

<center>    <img src="{{'assets/postResources/image-20200915101023288.png'|relative_url}}" alt="决策树的切分" />    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1.2 决策树的切分</div> </center>

![image-20200915101023288](../assets/postResources/image-20200915101023288.png)

假设决策树进行二路切分（正例/反例），左侧的决策树将实例集合切分成比较同质化的集合，而右侧的则似乎没起到分离的作用。

贪心的选择策略，应当是倾向于使结点上的数据同质化。为此需要测定信息的混杂度（熵）。构造树的过程，就是自顶向下递归地一边找当前情况下最好的属性，一边分类，直到样本分类的效果还不错，或者没有特征可选为止。

根据属性取值的不同，树节点的构造也有说法。

属性的取值可以是离散的(nominal)、有序的(Ordinal)、连续的(Continuous)。切分的分支可以是2路切分（简单）或是多路切分。

举例来说，大部分名词属性都是离散取值的，那就可以一个离散属性对应一路切分（多路，离散），比如颜色，可能是红黄蓝...或是把离散属性值分成两个子集，一边一个，按照样本属性的从属分到对应分支（2路，离散），比如{红黄}一组、{蓝绿}一组；有序属性也可以是一个属性值对应一个一路，比如衣服尺寸（小中大），或是属性值分成两个子集{小中}+{大}；连续的属性值可以通过离散化构造有序类属性或是用一个比较大小的不等式简单的二值切分。

> 把多个属性值划归两个子集再分类的过程可以根据经验调整。比如，对于有序属性，如果把属性值分成{小大}+{中}的组合，一般来说，恐怕效果不会很理想。



## <a id="2020/09/15">2020/09/15</a>

### <a id="信息熵">信息熵</a>

根据信息论，

> 信息是用来消除随机不确定性的东西。

因此，某件事情（信息？）发生所带来的信息量大小，就看其消除不确定性的程度。对于大概率的事件，比如”人被杀就会死“，其包含的信息很少，因为”人被杀就会死“的概率非常大，除了让听者觉得你智力有问题之外，从中得到不了太多信息。如果反过来说“人被杀也不会死”，这句话就很有信息量了。因为这一般是不会发生的，听者不禁会想，到底是这厮掌握了不朽的方法，还是这厮智力有问题？<del>（至少这也带来了信息）</del>

好吧，换个正常点的例子：你去宿舍楼下的地铁站的电子彩票售卖机里买福利彩票。花了10￥买一张，划开之后大喊：“噫，好了！我中了！”这句话就带来很多信息，因为一般来说买彩票是不会中的，在排除你撒谎的前提下，你中奖这一事件，否定了大概率的“你不会中奖”这一事件，消除了大量不确定性，带来了很多信息。

某事件的**信息量**为
$$
h(x) = -{log}_2{p(x)}
$$
信息量度量一个事件发生所带来的信息。事件越容易发生，其信息量越小。

随机变量的熵(Entropy)：

<center>    <img src="{{'assets/postResources/image-20200915101609249.png'|relative_url}}" alt="信息熵" />    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.1 信息熵</div> </center>

![image-20200915101609249](../assets/postResources/image-20200915101609249.png)

信息量度量一个具体事件发生带来的信息大小，而熵则是对所有可能产生信息量的期望值，也就是所有可能发生事件带来的信息量的期望。

打个比方，在最短编码情况下，对X=i分配$-{log}_2{P(x=i)}$位，则熵代表最短编码情况下的平均编码长度。随机变量可能性越多，概率越接近，其信息熵也就越大。



### <a id = "条件熵">条件熵</a>

随机变量X在给定条件Y = j下的**特定条件熵**H(X\|Y = j)，其定义类似于信息熵，只是要替换一下概率。

<center>    <img src="{{'assets/postResources/image-20200915103528148.png'|relative_url}}" alt="特定条件熵" />    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.2 特定条件熵</div> </center>

![image-20200915103528148](../assets/postResources/image-20200915103528148.png)

X在给定条件Y下的**条件熵**，相当于把所有y的可能取值对应的特定条件熵求期望。

<center>    <img src="{{'assets/postResources/image-20200915104144664.png'|relative_url}}" alt="条件熵" />    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.3 条件熵</div> </center>

![image-20200915104144664](../assets/postResources/image-20200915104144664.png)

容易理解，信息熵是大于某个条件下的条件熵的。毕竟如果已经获得了一个条件，那么原随机变量的变化可能性就减少了。可以极端假设一点：若Y = Ｘ，在已知Y的情况下，X可以说已经毫无信息可言了（已经知道了）。

称信息熵与某条件下的条件熵之差为**互信息**。之所以称为**互**信息，是因为X对于Y的互信息，也是Y对于X的互信息。

<center>    <img src="{{'assets/postResources/image-20200915105100530.png'|relative_url}}" alt="互信息" />    <br>    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.4 互信息</div> </center>

![image-20200915105100530](../assets/postResources/image-20200915105100530.png)

### 联合熵

联合熵用来度量一个联合分布的随机系统的不确定度。联合分布概率密度为$p(x,y)$的随机变量$(X,Y)$，其联合熵定义为
$$
H(X,Y) = -\sum_{x \in X}\sum_{y \in Y}p(x,y){log}{p(x,y)}
$$
熵的链式法则：
$$
设随机变量X_1,X_2,...,X_n服从p(x_1,x_2,...x_n),有
\\
H(X_1,X_2,...,X_n)=\sum^n_{i=1}H(X_i|X_{i-1},X_{i-2},...,X_1)
$$
根据链式法则有
$$
H(X,Y) = H(X) + H(Y|X)
$$
可以理解为：多个随机变量的随机系统，可以通过先观察一个随机变量获取信息量，在加上已知该随机变量基础上的条件信息量...当然，系统的信息量并不会因为观测的顺序而变化，因此也就有了互信息的等式。



### <a id="样本熵">样本熵</a>

设S为样本集，$P_+$为S中的正例比例，$P_-$为S中反例的比例（假设为2分类），可以根据$P_+$，$P_-$来描述样本的混杂度。
$$
H(S) = -P_+log_2P_+ - P_-log_2P_-
$$

两个样本比例越相近，样本熵越大。如果$P_+$，$P_-$中有为1或0的，则H(S) = 0。

### <a id="信息增益">信息增益</a>

$$
GAIN_{split} = Entropy(P) - \sum_{i=1}^k \frac{n_i}{n}Entropy(i)
$$

其中P是父节点，被切分成k个子节点。$n_i$是第i个子节点的样本数目。信息增益代表的是原信息熵与已知某条件后的信息熵之差，所以信息增益代表着决策树的目标类变量与点P对应的属性的互信息。信息增益也就是由于P节点对应切分带来的熵减，所以信息增益越大越好。

决策树学习中的信息增益概念，等价于训练数据集中label与特征的互信息。

### ID3

最早提出的决策树算法。ID3的思想，一言以蔽之，就是可着信息增益大的选。



## <a id="2020/09/17">2020/09/17</a>

### 过拟合和欠拟合

欠拟合(underfitting)就是模型的复杂度不够，没有提取出样本的特点，过拟合就是模型太精密了，甚至超出了样本属性原本的复杂特性。打个比方来说，就像炒菜干放盐和放了油盐酱醋味精花椒大料...的感觉。



### <a id="样本属性缺失">样本属性缺失</a>





#### 惩罚项（正则项）

参数较多时，往往容易使拟合出的曲线参数绝对值很大。为此，不妨在优化目标函数中增加一个惩罚项：
$$
\frac{λ}{2}||w||^2
$$
其中向量范数为2-范数。

调整λ的取值可以调整惩罚项的“惩罚力度”，



注意：模型参数多≠模型复杂，只能说明模型的能力更强。一般用模型参数向量的2-范数的大小来衡量模型复杂度。

### 概率



## <a id="2020/09/29">2020/09/29</a>

