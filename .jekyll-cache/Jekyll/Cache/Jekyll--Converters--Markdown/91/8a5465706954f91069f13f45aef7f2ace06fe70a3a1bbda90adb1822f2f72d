I"<p>这是在学校上机器学习课的个人笔记。
<del>本来在Typora上可以直接用“[TOC}”添加内容目录的，用jekyll就不行，使用jekyll-toc插件也有问题，淦矣</del></p>

<p>结果只好自己手动写链接锚点了。</p>

<p>注：涉及到图片的位置可能会发现一个不能正确加载的img标签，这是因为jekyll生成的目录<br /></p>

<h3 id="目录">目录</h3>

<ol>
  <li><strong><a href="#数学基础">数学基础</a></strong></li>
  <li><strong><a href="#2020/09/10">2020/09/10</a></strong>
<a href="#什么是机器学习">什么是机器学习</a>
<a href="#机器学习的一般泛型">机器学习的一般泛型</a>
<a href="#机器学习理论">机器学习理论</a>
<a href="#决策树">决策树</a></li>
  <li><a href="#2020/09/15"><strong>2020/09/15</strong></a>
<a href="#信息熵">信息熵</a>
<a href="#条件熵">条件熵</a>
<a href="#样本熵">样本熵</a></li>
  <li><strong><a href="#2020/09/17">2020/09/17</a></strong>
  <a href="#样本属性缺失">样本属性缺失</a></li>
</ol>

<p><br /></p>

<h2 id="数学基础"><a id="数学基础">数学基础</a></h2>

<p>数学真的很重要啊…可惜，大一大二的数学基础课程全都拿去考试用了。没办法，需要记一记的就记一记吧。</p>

<h3 id="线性代数">线性代数</h3>

<p>他妈的</p>

<h2 id="20200910"><a id="2020/09/10">2020/09/10</a></h2>

<h3 id="什么是机器学习"><a id="什么是机器学习">什么是机器学习</a></h3>

<p>研究计算机如何模拟/实现人的学习行为。</p>

<p>机器学习是人工智能的核心。其应用遍及NLP，对象识别，文本挖掘，生物信息学等。</p>

<p>给定样本集D={Xi，Yi}，输出空间（标记空间）Y</p>

<p>模型（也即函数）F(;θ)（也可作F(|θ)，在机器学习理论中不区分），有时也称学习器（learner）</p>

<h3 id="机器学习的一般泛型"><a id="机器学习的一般泛型">机器学习的一般泛型</a></h3>

<p><br /></p>

<p>一般的机器学习类型：</p>

<ul>
  <li>
    <p>监督学习
数据集的样本都带有标记</p>
  </li>
  <li>
    <p>无监督学习
数据集的样本无标记，一般用来做聚类学习</p>
  </li>
  <li>
    <p>强化学习
根据回报影响学习的策略</p>
  </li>
</ul>

<p><br /></p>

<p>根据输出空间的形式不同，可分成：</p>

<ul>
  <li>离散型集合Y如Y={+1，-1}，称为归类</li>
  <li>连续型集合Y如Y = R，称为回归</li>
</ul>

<p><br /></p>

<h3 id="机器学习理论"><a id="机器学习理论">机器学习理论：</a></h3>

<p>对于学习产生的模型（也可以说，函数F(;θ）），研究其</p>

<ol>
  <li>一致性</li>
  <li>偏执与方差</li>
  <li>采样复杂性</li>
  <li>学习率（学习速度）</li>
  <li>收敛性</li>
  <li>误差界</li>
  <li>稳定性</li>
</ol>

<p>……等等</p>

<p><br /></p>

<h3 id="机器学习研究的目的">机器学习研究的目的</h3>

<p>开发一种具有如下能力的系统：</p>

<ul>
  <li>表示</li>
  <li>分类，聚类，识别</li>
  <li>不确定条件下的推理（如电商app的商品推荐）</li>
  <li>预测</li>
  <li>对外界环境的反应</li>
</ul>

<p>经过学习获得的模型，对应关于数据的某种潜在规律，称为假设（hypothesis），而样本空间的潜在规律本身称为真相/真实（ground-truth）。假设与真相的关系，就好比社会主义和共产主义，永远无法到达，但可以不断接近。</p>

<p>可以把学习的过程看作在所有假设空间中搜索<code class="language-plaintext highlighter-rouge">function()</code>的过程。</p>

<h3 id="决策树"><a id="决策树">决策树</a></h3>

<p>一颗决策树可以表示输入属性的任何函数。</p>

<p>可以为每个样例建立一个从根到叶节点的路径，但是泛化能力不强，就像备考时将题目的答案背下来，而不会做其他题一样。因此应当找一个规模小一点的决策树，掌握样本的规律而非记录样本值。</p>

<p>对同一个训练数据集合，可能有多个决策树与之一致。</p>

<p>决策树能对数据进行<strong>切分</strong>。贪心的选择策略，应当是倾向于使结点上的数据同质化。为此需要测定信息的混杂度（熵）。
<span class="kdmath">$x^{23} y_{WhatTheFuck} \vec b \lim {(a+b)}$</span>
比如下面的例子：</p>

<p><br /></p>

<center>    <img src="/assets/postResources/image-20200915101023288.png" alt="决策树的切分" />    <br />    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图1.2 决策树的切分</div> </center>

<p><img src="../assets/postResources/image-20200915101023288.png" alt="image-20200915101023288" /></p>

<p>假设决策树进行二路切分（正例/反例），左侧的决策树将实例集合切分成比较同质化的集合，而右侧的则似乎没起到分离的作用。</p>

<p><br /></p>

<h2 id="20200915"><a id="2020/09/15">2020/09/15</a></h2>

<h3 id="信息熵"><a id="信息熵">信息熵</a></h3>

<p>随机变量的熵(Entropy)：H(X)是对从X随机采样值在最短编码情况下的平均编码长度。</p>

<center>    <img src="/assets/postResources/image-20200915101609249.png" alt="信息熵" />    <br />    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.1 信息熵</div> </center>

<p><img src="../assets/postResources/image-20200915101609249.png" alt="image-20200915101609249" /></p>

<p>这是因为根据信息论，在最短编码情况下，对X=i分配-logP(X=i)位(以2为底)。</p>

<h3 id="条件熵"><a id="条件熵">条件熵</a></h3>

<p>随机变量X在给定条件Y = v下的<strong>特定条件熵</strong>H(X|Y = v)，其定义类似于信息熵，只是要替换一下概率。</p>

<center>    <img src="/assets/postResources/image-20200915103528148.png" alt="特定条件熵" />    <br />    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.2 特定条件熵</div> </center>

<p><img src="../assets/postResources/image-20200915103528148.png" alt="image-20200915103528148" /></p>

<p>X在给定条件Y下的<strong>条件熵</strong>，相当于把所有y的可能取值对应的特定条件熵求期望。</p>

<center>    <img src="/assets/postResources/image-20200915104144664.png" alt="条件熵" />    <br />    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.3 条件熵</div> </center>

<p><img src="../assets/postResources/image-20200915104144664.png" alt="image-20200915104144664" /></p>

<p>容易理解，信息熵是大于某个条件下的条件熵的。毕竟如果已经获得了一个条件，那么原随机变量的变化可能性就减少了。可以极端假设一点：若Y = Ｘ，在已知Y的情况下，X可以说已经毫无信息可言了（已经知道了）。</p>

<p>称信息熵与某条件下的条件熵之差为<strong>互信息</strong>。之所以称为<strong>互</strong>信息，是因为X对于Y的互信息，也是Y对于X的互信息。</p>

<center>    <img src="/assets/postResources/image-20200915105100530.png" alt="互信息" />    <br />    <div style="color:orange; border-bottom: 1px solid #d9d9d9;    display: inline-block;    color: #999;    padding: 2px;">图2.4 互信息</div> </center>

<p><img src="../assets/postResources/image-20200915105100530.png" alt="image-20200915105100530" /></p>

<h3 id="样本熵"><a id="样本熵">样本熵</a></h3>

<p>设S为样本集，P+为S中的正例比例，P-为S中反例的比例，可以根据P+，P-来描述样本的混杂度。</p>

<h3 id="信息增益"><a id="信息增益">信息增益</a></h3>

<h2 id="20200917"><a id="2020/09/17">2020/09/17</a></h2>

<h3 id="样本属性缺失"><a id="样本属性缺失">样本属性缺失</a></h3>

:ET